# -*- coding: utf-8 -*-
"""Wiki-ngram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L_iMNzWzdcBeFb9n7mpSHIY2dSWKGNsL
The models is located at
    char model: https://drive.google.com/open?id=1-03h5MAcIw68DbvDI-pppyFzFvnchaYb
    word model: https://drive.google.com/open?id=1-36usEHXymSx-eeoJCpFED-GvjBzSB4R
    both of them on json format
    to get the next char after 3 chars we shoud write charModels[3]['ABC'], starts from 1 to 4 
    to get the next char after 1 word we shoud write wordModels[1]['ABC'], we only have bigrams
"""

from nltk.util import ngrams
from collections import Counter, defaultdict
import re
import string
from tqdm import tqdm
from copy import deepcopy
import json

num_word_models, num_char_models = 1, 4
wordModels = [0] + [defaultdict(lambda: defaultdict(lambda: 0)) for i in range(1,1+num_word_models)]
charModels = [0] + [defaultdict(lambda: defaultdict(lambda: 0)) for i in range(1,1+num_char_models)]
models = {'word':wordModels, 'char':charModels}

path = 'wiki-txt.txt'

# Count frequency of co-occurance  
with open(path) as fin:
  for sentence in tqdm(fin.readlines()):
    sentence = sentence.strip()
    sentence = sentence = re.sub(r'[^\u061F-\u066A|\s]', "", sentence)
    for i in range(1,1+num_char_models):
        for w in ngrams(sentence, i+1, pad_right=True, pad_left=True):
            charModels[i][w[:-1]][w[-1]] += 1

    for i in range(2,3):
        for w in ngrams(sentence.split(), i+1, pad_right=True, pad_left=True):
            wordModels[i][w[:-1]][w[-1]] += 1

# Let's transform the counts to probabilities
for model in tqdm(wordModels):
  if model == 0: continue
  for preWord in model:
      total_count = float(sum(model[preWord].values()))
      for w3 in model[preWord]:
          model[preWord][w3] /= total_count

# changedModels = deepcopy(charModels)
changedModels = wordModels

for m in changedModels:
  if m == 0: continue
  oldKeys = list(m.keys())
  for key in oldKeys:
    if type(key) is not str:
      try:
        m[str(' '.join(key))] = m[key]
      except:
        if key == None:
          m['*'] = m[key]
        else:
          newKey = ''
          for k in key:
            if k == None:
              newKey += '* '
            else:
              newKey += k + ' '
          m[newKey[:-1]] = m[key]
      del m[key]


with open('ngram_word.json', 'w') as fp:
    json.dump(changedModels, fp)

changedModels = charModels

for m in changedModels:
      if m == 0: continue
  oldKeys = list(m.keys())
  for key in oldKeys:
        if type(key) is not str:
          try:
        m[str(''.join(key))] = m[key]
      except:
        if key == None:
              m['*'] = m[key]
        else:
              newKey = ''
          for k in key:
                if k == None:
                  newKey += '*'
            else:
                  newKey += k
          m[newKey] = m[key]
      del m[key]


with open('ngram_char.json', 'w') as fp:
    json.dump(changedModels, fp)


# !cp ngram_word.json '/content/drive/My Drive/Old/ngram_word.json'
# !cp ngram_char.json '/content/drive/My Drive/Old/ngram_char.json'







# """Load txt"""

# from google.colab import drive
# drive.mount('/content/drive')
# !cp '/content/drive/My Drive/Old/wiki.txt' wiki-txt.txt

# !ls



# """Download Wiki and extract it"""
# """ To Google colab only """

# import sys
# from gensim.corpora import WikiCorpus
# from tqdm import tqdm

# def make_corpus(in_f, out_f):

# 	"""Convert Wikipedia xml dump file to text corpus"""

# 	output = open(out_f, 'w')
# 	wiki = WikiCorpus(in_f)

# 	i = 0
# 	for text in tqdm(wiki.get_texts()):
# 		output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
# 		i = i + 1
# 		if (i % 10000 == 0):
# 			print('Processed ' + str(i) + ' articles')
# 	output.close()
# 	print('Processing complete!')

# !wget "https://dumps.wikimedia.org/arwiki/20191020/arwiki-20191020-pages-articles.xml.bz2"

# make_corpus("arwiki-20191020-pages-articles.xml.bz2", "wiki-txt.txt")
# !head -5 wiki-txt.txt >> small-wiki-txt.txt
# # 571038it

# # from google.colab import drive
# # drive.mount('/content/drive')
# # !cp wiki-txt.txt '/content/drive/My Drive/Old/wiki.txt'















